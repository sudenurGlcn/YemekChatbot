
import pandas as pd
import faiss
import pickle
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

# CSV'den intent verisini yÃ¼kle
df = pd.read_csv(r"D:\Chatbot\yeniIntents.csv")

# Embedding modeli
embed_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
doc_embeddings = embed_model.encode(df["text"].tolist(), convert_to_numpy=True)

# FAISS index oluÅŸtur ve veriyi ekle
index = faiss.IndexFlatL2(doc_embeddings.shape[1])
index.add(doc_embeddings)

# FAISS index ve dÃ¶kÃ¼manlarÄ± kaydet
faiss.write_index(index, "faiss_index.index")
with open("docs.pkl", "wb") as f:
    pickle.dump((df["text"].tolist(), df["response"].tolist()), f)

print("âœ… Embed + FAISS + DÃ¶kÃ¼manlar HazÄ±r")

# CUDA kontrolÃ¼
if torch.cuda.is_available():
    print("ğŸš€ CUDA destekli sistem tespit edildi (GPU kullanÄ±lacak)")
else:
    print("âš ï¸ GPU bulunamadÄ±, CPU kullanÄ±lacak")

# Model adÄ±
model_name = "NousResearch/Nous-Hermes-2-Mistral-7B-DPO"

# 4-bit quantization ayarlarÄ±
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

# Tokenizer ve model yÃ¼kle
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
    torch_dtype=torch.float16
)
model.eval()

# RAG fonksiyonu: intent eÅŸleÅŸmesi + LLM cevabÄ±
def rag_cevapla(soru, top_k=1):
    # FAISS ve dokÃ¼manlarÄ± yÃ¼kle
    index = faiss.read_index("faiss_index.index")
    with open("docs.pkl", "rb") as f:
        docs, responses = pickle.load(f)

    # Embed modeli (yeniden yÃ¼kle)
    embed_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    query_vec = embed_model.encode([soru], convert_to_numpy=True)

    # FAISS arama
    D, I = index.search(query_vec, top_k)

    # En yakÄ±n intent ve cevabÄ±
    yakin_intentler = [docs[i] for i in I[0]]
    yakin_cevaplar = [responses[i] for i in I[0]]

    # Prompt oluÅŸtur
    en_yakin_bilgi = "\n".join(yakin_cevaplar)
    prompt = f"""Sen bir yemek danÄ±ÅŸmanÄ±sÄ±n. TÃ¼rk ve dÃ¼nya mutfaÄŸÄ± hakkÄ±nda tarif, malzeme ve piÅŸirme yÃ¶ntemlerini aÃ§Ä±klarsÄ±n. Sorulara sade, anlaÅŸÄ±lÄ±r ve doÄŸru cevaplar ver.

### Soru:
{soru}

### Bilgi:
{en_yakin_bilgi}

### Cevap:"""

    # Tokenize et
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Modelden yanÄ±t Ã¼ret
    output = model.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=False,
        temperature=0.3,
        top_p=0.9,
        num_beams=4,
        early_stopping=True
    )

    cevap = tokenizer.decode(output[0], skip_special_tokens=True)
    if "### Cevap:" in cevap:
        cevap = cevap.split("### Cevap:")[-1].strip()

    # SonuÃ§larÄ± dÃ¶ndÃ¼r
    return {
        "en_yakin_intent": yakin_intentler[0],
        "en_yakin_cevap": yakin_cevaplar[0],
        "model_cevabi": cevap
    }

# Ã–rnek Soru
soru = "Merhaba"
sonuc = rag_cevapla(soru)

# Terminal Ã§Ä±ktÄ±sÄ±
print("ğŸ‘¤ Soru:", soru)
print("ğŸ“Œ En YakÄ±n Intent:", sonuc["en_yakin_intent"])
print("ğŸ“‹ HazÄ±r Intent CevabÄ±:", sonuc["en_yakin_cevap"])
print("ğŸ¤– LLM Model CevabÄ±:", sonuc["model_cevabi"])
